# SuperJobParsing
1. При написании парсера я использовал гибридный подход, так как нашёл официальное API. Для поиска самих вакансий, перехода по страницам и получения их id и заголовков я использовал Scrapy, для получения номеров телефонов я использовал API, так как нужна авторизация на сайте для получения контактных данных. Как получить больше вакансий, чем доступно на 50 страницах на сайте я написал в комментарии в файле vacancies.py.
2. Для хранения данных в формате JSON я бы использовал MongoDB, этому способствует документоориентированность базы данных. MongoDB позволяет хранить JSON без определенной схемы и выполнять поиск с помощью индексов. Однако же если JSON хорошо структурированы и нам не нужна гибкость БД, можно подумать и о Postgres с ElasticSearch(им я не пользовался, но судя по тому что я изучил - позволяет сильно оптимизировать поиск).
3. Алгоритм для имён находится в файле names_algorythm.py
